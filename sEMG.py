import os
from collections import defaultdict
import numpy as np
import scipy.io
from scipy.signal import butter, iirnotch, filtfilt
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torch.utils.tensorboard import SummaryWriter
import yaml
from model import EMG2DCNN
from datetime import datetime

# -----------------------------
# æ»¤æ³¢å™¨
# -----------------------------
def notch_filter(signal, fs=200, freq=50, Q=30):
    b, a = iirnotch(freq/(fs/2), Q)
    return filtfilt(b, a, signal, axis=0)

def bandpass_filter(signal, fs=200, low=20, high=500, order=4):
    nyq = fs / 2
    if high >= nyq:
        high = nyq * 0.99
    b, a = butter(order, [low/nyq, high/nyq], btype='band')
    return filtfilt(b, a, signal, axis=0)

# -----------------------------
# æ•°æ®åŠ è½½
# -----------------------------
def load_subject_data(subject_dir, experiment, fs, filters, use_restimulus=False):
    emg_list = []
    label_list = []
    repetition_list = []

    if experiment == "ALL":
        mat_files = sorted([f for f in os.listdir(subject_dir) if f.endswith('.mat')])
    else:
        mat_files = sorted([f for f in os.listdir(subject_dir) if f.endswith('.mat') and experiment in f])

    print(f"[{subject_dir}] Found {len(mat_files)} files for {experiment}")

    for f in mat_files:
        path = os.path.join(subject_dir, f)
        data = scipy.io.loadmat(path)
        emg = data['emg']

        if use_restimulus:
            stimulus = data['restimulus'].flatten()
            repetition = data['rerepetition'].flatten()
        else:
            stimulus = data['stimulus'].flatten()
            repetition = data['repetition'].flatten()

        # ğŸš€ ç»™æ ‡ç­¾åšåç§»ï¼Œé˜²æ­¢é‡å 
        if experiment != "ALL":
            offset = 0
        elif "E1" in f:
            offset = 0
        elif "E2" in f:
            offset = 12
        elif "E3" in f:
            offset = 12+17
        else:
            offset = 0
        stimulus = stimulus + offset

        # æ»¤æ³¢
        notch_freq = filters["notch"]["freq"]
        notch_Q = filters["notch"]["Q"]
        bandpass_low = filters["bandpass"]["low"]
        bandpass_high = filters["bandpass"]["high"]
        bandpass_order = filters["bandpass"]["order"]

        emg = notch_filter(emg, fs=fs, freq=notch_freq, Q=notch_Q)
        emg = bandpass_filter(emg, fs=fs, low=bandpass_low, high=bandpass_high, order=bandpass_order)

        emg_list.append(emg)
        label_list.append(stimulus)
        repetition_list.append(repetition)

    if not emg_list:
        return None, None, None

    all_emg = np.vstack(emg_list)
    all_labels = np.concatenate(label_list)
    all_repetitions = np.concatenate(repetition_list)
    return all_emg, all_labels, all_repetitions


# -----------------------------
# æ ·æœ¬æå–
# -----------------------------
def extract_samples(emg, labels, repetitions, window=200, step=20):
    samples = defaultdict(list)
    current_label = labels[0]
    current_repetition = repetitions[0]
    start_idx = 0

    for i in range(1, len(labels)):
        if labels[i] != current_label or repetitions[i] != current_repetition:
            segment_emg = emg[start_idx:i]
            segment_rep = repetitions[start_idx:i]
            if len(segment_emg) >= window:
                for j in range(0, len(segment_emg)-window+1, step):
                    clip = segment_emg[j:j+window]
                    clip_rep = segment_rep[j + window//2]
                    samples[current_label].append((clip, clip_rep))
            current_label = labels[i]
            current_repetition = repetitions[i]
            start_idx = i

    segment_emg = emg[start_idx:]
    segment_rep = repetitions[start_idx:]
    if len(segment_emg) >= window:
        for j in range(0, len(segment_emg)-window+1, step):
            clip = segment_emg[j:j+window]
            clip_rep = segment_rep[j + window//2]
            samples[current_label].append((clip, clip_rep))

    return samples

# -----------------------------
# Dataset
# -----------------------------
class EMGDataset(Dataset):
    def __init__(self, X, y):
        tensor_X = torch.tensor(X, dtype=torch.float32)
        self.X = tensor_X.unsqueeze(1)
        self.y = torch.tensor(y, dtype=torch.long)

    def __len__(self):
        return len(self.y)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

# -----------------------------
# è®­ç»ƒå’ŒéªŒè¯
# -----------------------------
def train_epoch(model, loader, optimizer, criterion, device):
    model.train()
    total_loss, correct, total = 0, 0, 0
    for X, y in loader:
        X, y = X.to(device), y.to(device)
        optimizer.zero_grad()
        out = model(X)
        loss = criterion(out, y)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
        pred = out.argmax(1)
        correct += (pred == y).sum().item()
        total += y.size(0)
    return total_loss / len(loader), correct / total

def eval_epoch(model, loader, criterion, device):
    model.eval()
    total_loss, correct, total = 0, 0, 0
    with torch.no_grad():
        for X, y in loader:
            X, y = X.to(device), y.to(device)
            out = model(X)
            loss = criterion(out, y)
            total_loss += loss.item()
            pred = out.argmax(1)
            correct += (pred == y).sum().item()
            total += y.size(0)
    return total_loss / len(loader), correct / total

# -----------------------------
# ä¸»å‡½æ•°
# -----------------------------
def main(cfg):
    root_dir = cfg["data"]["root_dir"]
    fs = cfg["data"]["fs"]
    window = cfg["data"]["window"]
    step = cfg["data"]["step"]
    experiment = cfg["experiment"]["name"]
    use_restimulus = cfg["labels"]["use_restimulus"]
    note = cfg["experiment"]["note"]
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
    safe_note = note.replace(" ", "_").replace("/", "_") if note else "no_note"

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = os.path.join(cfg["experiment"]["log_dir"], f"{experiment}_{safe_note}_{timestamp}")
    os.makedirs(output_dir, exist_ok=True)

    # TensorBoardç›®å½•
    tb_dir = os.path.join(output_dir, "tensorboard")
    os.makedirs(tb_dir, exist_ok=True)

    # æ¨¡å‹ä¿å­˜ç›®å½•
    model_dir = os.path.join(output_dir, "checkpoints")
    os.makedirs(model_dir, exist_ok=True)

    writer = SummaryWriter(log_dir=tb_dir)

    all_emg = []
    all_labels = []
    all_repetitions = []
    subjects = sorted([d for d in os.listdir(root_dir) if d.startswith('s') and os.path.isdir(os.path.join(root_dir, d))])

    print(f"Selected experiment: {experiment}")
    print(f"Found subjects: {subjects}")

    for subj in subjects:
        emg, labels, repetitions = load_subject_data(
            os.path.join(root_dir, subj),
            experiment=experiment,
            fs=fs,
            filters=cfg["filters"],
            use_restimulus=use_restimulus
        )
        if emg is None:
            continue
        all_emg.append(emg)
        all_labels.append(labels)
        all_repetitions.append(repetitions)

    all_emg = np.vstack(all_emg)
    all_labels = np.concatenate(all_labels)
    all_repetitions = np.concatenate(all_repetitions)

    # æ–°å¢è¿‡æ»¤ï¼Œå»æ‰æ ‡ç­¾ä¸º0çš„æ•°æ®
    mask = all_labels > 0
    all_emg = all_emg[mask]
    all_labels = all_labels[mask]
    all_repetitions = all_repetitions[mask]

    unique_labels = np.unique(all_labels)
    print(f"Detected gesture classes (excluding 0): {unique_labels}")

    label_map = {label: idx for idx, label in enumerate(unique_labels)}
    mapped_labels = np.array([label_map[l] for l in all_labels])

    from collections import Counter

    label_counts = Counter(mapped_labels)
    print("mapped_labels ä¸­å„ç±»åˆ«æ ·æœ¬æ•°é‡ï¼š")
    for label, count in label_counts.items():
        print(f"ç±»åˆ« {label}: {count} ä¸ªæ ·æœ¬")

    samples = extract_samples(
        all_emg,
        mapped_labels,
        all_repetitions,
        window=window,
        step=step
    )

    num_classes = len(unique_labels)
    print(f"Number of classes: {num_classes}")

    X_train, y_train, X_test, y_test = [], [], [], []
    for c in range(num_classes):
        c_samples = samples[c]
        if len(c_samples) == 0:
            continue
        train_samples = [clip for clip, rep in c_samples if rep in [1,2,3,4]]
        test_samples = [clip for clip, rep in c_samples if rep in [5,6]]

        if len(train_samples) == 0 or len(test_samples) == 0:
            print(f"âš ï¸ Class {c} skipped due to insufficient samples")
            continue

        X_train += train_samples
        y_train += [c] * len(train_samples)
        X_test += test_samples
        y_test += [c] * len(test_samples)

    if len(X_train) == 0 or len(X_test) == 0:
        print("æ•°æ®ä¸è¶³ï¼Œæ— æ³•è®­ç»ƒ")
        return

    train_ds = EMGDataset(X_train, y_train)
    test_ds = EMGDataset(X_test, y_test)
    # ä¿å­˜æµ‹è¯•æ•°æ®ä¾›C++ç«¯éªŒè¯
    X_test_tensor = test_ds.X.cpu().numpy()
    y_test_tensor = test_ds.y.cpu().numpy()

    export_dir = r"D:\Project\sEMG\test_data"
    exp_name = cfg["experiment"]["name"]
    os.makedirs(export_dir, exist_ok=True)

    np.save(os.path.join(export_dir, f"{exp_name}_X.npy"), X_test_tensor)
    np.save(os.path.join(export_dir, f"{exp_name}_y.npy"), y_test_tensor)

    print(f"âœ… æµ‹è¯•æ•°æ®å·²åˆ†åˆ«å¯¼å‡º: {export_dir}")

    train_loader = DataLoader(train_ds, batch_size=cfg["train"]["batch_size"], num_workers=cfg["train"]["num_workers"], shuffle=True)
    test_loader = DataLoader(test_ds, batch_size=cfg["train"]["batch_size"], num_workers=cfg["train"]["num_workers"])

    model = EMG2DCNN(
        input_shape=(1, window, all_emg.shape[1]),
        model_cfg=cfg["model"],
        num_classes=num_classes
    ).to(device)

    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=cfg["train"]["learning_rate"], weight_decay=cfg["train"]["weight_decay"])

    # å†™å…¥é…ç½®
    config_str = yaml.dump(cfg, allow_unicode=True, sort_keys=False)
    writer.add_text("Experiment/Config", f"```yaml\n{config_str}\n```")
    if note:
        writer.add_text("Experiment/Note", note)

    # æŠŠé…ç½®ä¿å­˜åˆ°æ¨¡å‹ç›®å½•
    config_path = os.path.join(model_dir, "config.yaml")
    with open(config_path, "w", encoding="utf-8") as f:
        f.write(config_str)

    epochs = cfg["train"]["epochs"]
    for epoch in range(1, epochs + 1):
        tr_loss, tr_acc = train_epoch(model, train_loader, optimizer, criterion, device)
        val_loss, val_acc = eval_epoch(model, test_loader, criterion, device)
        print(f"[{experiment}] Epoch {epoch:03d} | Train Loss: {tr_loss:.4f} Acc: {tr_acc*100:.1f}% | Val Loss: {val_loss:.4f} Acc: {val_acc*100:.1f}%")
        writer.add_scalar("Loss/Train", tr_loss, epoch)
        writer.add_scalar("Loss/Validation", val_loss, epoch)
        writer.add_scalar("Accuracy/Train", tr_acc, epoch)
        writer.add_scalar("Accuracy/Validation", val_acc, epoch)

        # ä¿å­˜æ¯ä¸ªepochæ¨¡å‹
        if epoch % 5 == 0 or epoch == epochs:
            model_path = os.path.join(model_dir, f"epoch_{epoch:03d}.pt")
            torch.save(model.state_dict(), model_path)

    writer.close()


if __name__ == "__main__":
    # with open("config.yaml", "r", encoding="utf-8") as f:
    #     cfg = yaml.safe_load(f)
    #
    # main(cfg)

    import numpy as np

    # åŠ è½½åŸå§‹æ•°æ®
    data = np.load('test_data_old/E1_X.npy')  # å½¢çŠ¶ [N, C, H, W]

    # åˆ›å»ºå­˜å‚¨ç›®å½•
    import os

    os.makedirs('calib_data', exist_ok=True)

    # é€æ ·æœ¬ä¿å­˜ä¸ºå•ç‹¬çš„.npyæ–‡ä»¶
    for i in range(data.shape[0]):
        sample = data[i]  # è·å–ç¬¬iä¸ªæ ·æœ¬ [C, H, W]
        np.save(f'calib_data/{i}.npy', sample)  # ä¿å­˜ä¸º 0.npy, 1.npy, ...

    print(f"å·²æ‹†åˆ† {data.shape[0]} ä¸ªæ ·æœ¬åˆ° calib_data/ ç›®å½•")
    with open('calibration.txt', 'w') as f:
        for i in range(data.shape[0]):
            f.write(f'calib_data/{i}.npy\n')

    print("å·²ç”Ÿæˆ calibration.txt æ–‡ä»¶")